1.What is the significance of Exploratory Data Analysis (EDA)
Exploratory Data Analysis (EDA) is an approach to analyzing and understanding data that involves
summarizing and visualizing the data, identifying patterns and relationships, and testing
hypotheses. It is an important step in the data analysis process because it helps you get to know
your data, uncover insights, and identify potential issues or problems. EDA is especially useful
when you are working with large or complex datasets, or when you are trying to answer openï¿¾ended questions. It can help you identify trends, patterns, and outliers in the data, as well as
understand the underlying structure of the data. By performing EDA, you can gain a better
understanding of the data and generate ideas for further analysis or modeling. EDA is typically a
preliminary step in the data analysis process, before more formal statistical analysis or modeling is
conducted. It is an iterative process, and you may need to perform multiple rounds of EDA to fully
understand the data and identify all relevant patterns and relationships.
#########################################################################################################
2.How can you add a column to a Pandas Data Frame? *import pandas as pd data =
pd.DataFrame({ 'id': [ 101, 123, 139, 112, 133], 'age': [ 10, 12, 13, 11, 12], 'gender': [ 'M', 'F', 'F', 'M',
'M'], 'group': [ 'first', 'second', 'first', 'third', 'third'], 'math_score': [ 41.5, 43, 38, 47, 29.5] }) data
series = pd.Series([40, 38, 32.5, 27, 30]) data['science_score'] = series.values data
#################################################################################################
3.How do you treat outliers in a dataset? Outliers are observations that are significantly different
from the majority of the data in a dataset. There are several ways to treat outliers in a dataset:
Ignore the outlier: If the outlier is a result of a data recording error or is not representative of the
underlying distribution, it may be appropriate to simply ignore the outlier. Transform the data: Some
outliers may be the result of data that is skewed or has a non-normal distribution. In these cases,
transforming the data using a method such as log transformation can help to reduce the influence
of the outlier. Impute the outlier: If the outlier is a result of a missing value or error, it may be
possible to impute the value using a method such as mean imputation or linear regression
imputation. Winsorize the data: Winsorization involves replacing extreme values with a less
extreme value, such as the 95th or 99th percentile. This can help to reduce the influence of outliers
on the statistical analysis. Use robust statistical methods: Some statistical methods, such as the
median and interquartile range, are less sensitive to the presence of outliers than others. Using
these methods can help to mitigate the influence of outliers on the analysis. It is important to
carefully consider the appropriate method for treating outliers in a dataset, as simply removing
them without careful consideration can potentially bias the results of the analysis.
import pandas as pd import scipy.stats as stats import matplotlib.pyplot as plt df=
pd.read_excel("C:/Users/91988/Desktop/AI LAB/Book1.xlsx") df
fig = plt.figure(figsize =(6, 3)) plt.boxplot(df['Weight']) plt.show()
##################################################################################################
4.Differentiate between Covariance and Correlation? Explain in detail Covariance and correlation
are two statistical measures that are used to describe the relationship between two variables.
Covariance is a measure of the degree to which two variables change together. It is calculated by
12/28/22, 8:24 PM push ongithub - Jupyter Notebook
localhost:8888/notebooks/push ongithub.ipynb 2/6
taking the product of the deviations of each variable from its mean, and dividing the result by the
number of observations. A positive covariance indicates that the variables are positively related,
meaning that they tend to increase or decrease together. A negative covariance indicates that the
variables are negatively related, meaning that one variable tends to increase while the other tends
to decrease. Correlation, on the other hand, is a standardized measure of the relationship between
two variables. It is calculated by dividing the covariance of the two variables by the product of their
standard deviations. The resulting value is a number between -1 and 1, where -1 indicates a
perfect negative correlation, 0 indicates no correlation, and 1 indicates a perfect positive
correlation. One key difference between covariance and correlation is that covariance is sensitive
to the scale of the variables, while correlation is not. This means that a change in the scale of the
variables can affect the covariance, but it will not affect the correlation. Another difference is that
covariance is not always easy to interpret, because it depends on the units of the variables.
Correlation, on the other hand, is easier to interpret because it is standardized and always falls
within a fixed range (-1 to 1). In summary, covariance is a measure of the relationship between two
variables that is sensitive to the scale of the variables and may be difficult to interpret, while
correlation is a standardized measure of the relationship between two variables that is not
sensitive to the scale of the variables and is easier to interpret.
#############################################################################################
5. What are the challenges involved in Data Integration Data integration involves combining data
from different sources and making it available for analysis and use. Some of the challenges
involved in data integration include: Data heterogeneity: Different data sources often use
different data models, schemas, and formats, which can make it difficult to integrate their data.
Data quality: Data from different sources may be incomplete, inaccurate, or inconsistent,
which can affect the quality and reliability of the integrated data. Data security: Ensuring the
security and privacy of data from different sources can be a challenge when integrating data.
Data governance: Managing and governing the integration of data from different sources can
be complex, particularly when there are multiple stakeholders involved. Scalability: Integrating
large volumes of data from multiple sources can be a challenge in terms of performance and
scalability. Real-time integration: In some cases, data needs to be integrated in real-time,
which can be a challenge due to the complexity and volume of the data. Integration with
legacy systems: Integrating data from legacy systems can be difficult due to the outdated
technologies and systems involved.
##################################################################################################3
6.Two dias are thrown simultaneously,and the sum of the number obtained found to be 7.What is
the probability that the number 3 has appeared st least one?
Two dias are thrown simultaneously,So total number of possible outcomes=36 (i)Sum of the nuber
appearing on the dice is 7.
So,the possible ways are(1,6),(2,5),(3,4),(4,3),(5,2)and(6,1) Number of possible ways=6 number of
possible 3 coming=2/6
####################################################################################################
(7) (i)Spot outliers in Income using bivariate plot import matplotlib.pyplot as plt df =
pd.read_csv("credit.csv") plt.scatter(df["Income"], df["AnotherVariable"]) plt.show()
12/28/22, 8:24 PM push ongithub - Jupyter Notebook
localhost:8888/notebooks/push ongithub.ipynb 3/6
(ii)Spot outliers in any one feature using box plot import matplotlib.pyplot as plt import seaborn as
sns df = sns.load_dataset("credit") sns.boxplot(x=df["balance"]) plt.show() outliers =
sns.get_outliers(df["balance"]) print(outliers)
(iii) Spot outliers using Histogram plot import pandas as pd import matplotlib.pyplot as plt df =
pd.read_csv('credit.csv') balance = df['balance'] plt.hist(balance, bins=50) plt.show() q1, q3 =
balance.quantile([0.25, 0.75]) iqr = q3 - q1 outliers = balance[(balance < q1 - 1.5 * iqr) | (balance >
q3 + 1.5 * iqr)] print(f'Number of outliers: {len(outliers)}')
(iv) Detect outliers in any one feature using IQR method import numpy as np Q1 =
np.percentile(feature, 25) Q3 = np.percentile(feature, 75) IQR = Q3 - Q1 lower_outlier = Q1 - 1.5 *
IQR upper_outlier = Q3 + 1.5 * IQR outliers = [] for value in feature: if value < lower_outlier or value
> upper_outlier: outliers.append(value) print(outliers)
(v) Detect outliers using percentile capping (Winsorization method) import numpy as np credit_data
= np.loadtxt('credit.csv', delimiter=',') lower_limit = np.percentile(credit_data, 5) upper_limit =
np.percentile(credit_data, 95) credit_data[credit_data < lower_limit] = lower_limit
credit_data[credit_data > upper_limit] = upper_limit
(vi) Treat outliers by Deleting observations Identify the outliers in the dataset. There are several
ways to do this, such as using statistical methods like the interquartile range (IQR) or visually
inspecting box plots or scatter plots.
Determine how to define an outlier. You can use a standard definition, such as values that fall
outside of 1.5 times the IQR, or you can set your own threshold based on the context and the
goals of your analysis.
Remove the outliers from the dataset. You can do this by selecting all rows that do not meet your
definition of an outlier and creating a new dataset without those rows.
Consider the impact of removing the outliers. Removing outliers can significantly alter the
distribution and statistical properties of your data, so it is important to carefully consider the
implications of this decision. You may want to try multiple approaches and compare the results to
determine the best approach for your analysis.
It is important to note that this is just one possible approach to dealing with outliers in a credit
dataset. There are many other methods that you could consider, such as imputing missing values
or transforming the data. Ultimately, the best approach will depend on the specific goals of your
analysis and the characteristics of your data.
(vii) Treat outliers using imputation (mean, median and zero) import pandas as pd df =
pd.read_csv('credit.csv') Q1 = df.quantile(0.25) Q3 = df.quantile(0.75) IQR = Q3 - Q1 outliers =
df[((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)] imputation_method = 'mean' if
12/28/22, 8:24 PM push ongithub - Jupyter Notebook
localhost:8888/notebooks/push ongithub.ipynb 4/6
imputation_method == 'mean': df.loc[outliers.index] = df.loc[outliers.index].fillna(df.mean()) elif
imputation_method == 'median': df.loc[outliers.index] = df.loc[outliers.index].fillna(df.median()) elif
imputation_method == 'zero': df.loc[outliers.index] = df.loc[outliers.index].fillna(0)
#####################################################################################################3
8.For the titanic dataset perform the following: i.Display the number of missing values for each
feature in the dataset import numpy as np import pandas as pd import matplotlib.pyplot as plt
import seaborn as sns import missingno as msno
df=pd.read_csv('C:/Users/k2060/Documents/AI&ML/archive (k)/tested.csv') msno.bar(df)
msno.matrix(df)
df.dropna(inplace=True) df.isnull().sum()
del df['Age'] del df['Cabin'] df.head(10)
df.info()
df=pd.read_csv('C:/Users/k2060/Documents/AI&ML/archive (k)/tested.csv') print("before
imputation\n") print(df.isnull().sum())
print("after imputation\n") df['Cabin']=df['Cabin'].fillna(df['Cabin'].mode()[0])
df['Age']=df['Age'].fillna(df['Age'].mode()[0]) print(df.isnull().sum())
print("Null values ") print(df.isnull().sum()) print("count of Null values after imputation")
df['Age']=df['Age'].fillna(df['Age'].mean()) print(df.isnull().sum())
df.head().isna()
import pandas as pd df=pd.read_csv("C:/Users/k2060/Documents/AI&ML/archive (k)/tested.csv")
df['Age'].fillna(method='ffill',limit=1,inplace=True) df['Cabin'].fillna(method='bfill',inplace=True)
df['Cabin'].fillna(method='bfill',inplace=True)
df.head().isna()
import pandas as pd import numpy as np import matplotlib.pyplot as plt
df=pd.DataFrame({'Company':['HP','HP','DELL','DELL','FB','FB'], 'Person':
['Richard','Angela','Mary','Rick','Julla','Kevin'], 'Sales':[2000,1200,3400,1245,2430,3500]}) df
company = df.groupby("Company")["Sales"].mean() highest_sales_company = company.idxmax()
print(highest_sales_company)
sales_stats = df["Sales"].describe() sales_stats_transposed = sales_stats.transpose()
print(sales_stats_transposed)
##############################################################################################
9.Using the below Pandas data frame, find the company with the highest average sales. Derive the
summary statistics for the sales column and transpose the statistics.
create a dataframe comany * person * sales
12/28/22, 8:24 PM push ongithub - Jupyter Notebook
localhost:8888/notebooks/push ongithub.ipynb 5/6
import pandas as pd df = pd.read_csv("sales_data.csv") sales_by_company =
df.groupby("Company")["Sales"].mean() highest_sales_company = sales_by_company.idxmax()
print(highest_sales_company)
sales_stats = df["Sales"].describe() sales_stats_transposed = sales_stats.transpose()
print(sales_stats_transposed)
##################################################################################3
10.10. Problem statement a. Detect outliers using z score method import pandas as pd df =
pd.read_csv("C:/Users/k2060/Documents/AI&ML/archive (k)/tested.csv") mean = df['Age'].mean()
std = df['Age'].std() df['z_score'] = (df['Age'] - mean) / std threshold = 3 outliers = df[(df['z_score'] >
threshold) | (df['z_score'] < -threshold)] print(outliers)
b. Detect outliers using IQR method age = df['Age'] Q1 = age.quantile(0.25) Q3 =
age.quantile(0.75) IQR = Q3 - Q1 lower_bound = Q1 - 1.5 * IQR upper_bound = Q3 + 1.5 * IQR
outliers = age[(age < lower_bound) | (age > upper_bound)] print(outliers)
c. Detect outliers using percentile capping (WINSORIZATION method) Q1 =
df["Age"].quantile(0.25) Q3 = df["Age"].quantile(0.75) IQR = Q3 - Q1 lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR df["Age"] = df["Age"].clip(lower=lower_bound, upper=upper_bound)
Q1 = df["Fare"].quantile(0.25) Q3 = df["Fare"].quantile(0.75) IQR = Q3 - Q1 lower_bound = Q1 -
1.5 * IQR upper_bound = Q3 + 1.5 * IQR df["Fare"] = df["Fare"].clip(lower=lower_bound,
upper=upper_bound)
d. Spot outliers using Box and whisker plot (box plot). sns.boxplot(x=df['Fare'])
e. Spot outlier using Scatter plot. sns.boxplot(x=df['Age'])
f. Spot outlier using Histogram plot plt.scatter(df['Age'],df['Fare']) plt.xlabel('Age') plt.ylabel('Fare')
plt.show()
g. Spot outlier using Distribution Plot. plt.hist(df['Age']) plt.xlabel('Age') plt.show()
h. Treat outliers using Deleting observations, transforming values (scaling and cube root
transformation) and imputation (mean, median and zero) sns.displot(df['Age']) plt.xlabel('Age')
plt.show()
######################################################
11.
###########################################################3
12. How to handle the missing values in the dataset? Explain.
There are several ways to handle missing values in a dataset: Delete rows or columns: If a row or
column has a large number of missing values, it may be best to delete it altogether. This is
especially true if the missing values are not at random, because they may introduce bias into the
analysis. Impute missing values: This involves replacing the missing values with some estimate of
the true value. There are several ways to do this, including replacing the missing values with the
mean or median of the other values, or using more advanced techniques such as multiple
imputation or matrix completion. Predict missing values: Another option is to build a model to
predict the missing values based on the other values in the dataset. This can be done using
machine learning algorithms such as linear regression or random forests. Leave the missing
12/28/22, 8:24 PM push ongithub - Jupyter Notebook
localhost:8888/notebooks/push ongithub.ipynb 6/6 g g g g
values as-is: In some cases, it may be appropriate to leave the missing values as-is and simply
analyze the data that is available. This is often the case when working with time series data, where
missing values may be due to the fact that the data was not collected at certain points in time.
##############################################################################################3
13.The statistical summary of Iris dataset is as follows.Analyse and explain statistical metrics from
above summary.
import pandas as pd import numpy as np df=pd.read_csv("C:/Users/91988/Downloads/archive
(11)/Iris.csv") df
df.describe()
###################################################################################
14.. Describe univariate, bivariate, and multivariate analysis with suitable examples.
univariate covarience import pandas as pd x=pd.Series([1,2,5,10,20]) y=pd.Series([1,2,5,-10,12])
print("the covarience value:",x.cov(y))
univariate correlation import pandas as pd x=pd.Series([1,2,5,10,20]) y=pd.Series([1,2,5,-10,12])
print("the correlation value:",x.corr(y))
bivariate coverience import pandas as pd import numpy as np
df=pd.read_csv("C:/Users/91988/Downloads/archive (11)/Iris.csv") df
df['SepalLengthCm'].cov(df['PetalWidthCm'])
bivariate correlation import pandas as pd import numpy as np
df=pd.read_csv("C:/Users/91988/Downloads/archive (11)/Iris.csv") df
df['SepalLengthCm'].corr(df['PetalWidthCm'])
multivariate coverience import pandas as pd import numpy as np
df=pd.read_csv("C:/Users/91988/Downloads/archive (11)/Iris.csv") df df.cov()
multivariate correlation import pandas as pd import numpy as np
df=pd.read_csv("C:/Users/91988/Downloads/archive (11)/Iris.csv") df df.corr()
###########
